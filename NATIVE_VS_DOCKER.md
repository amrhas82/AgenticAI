# Native Mode vs Docker Mode - Explained

## üéØ **Quick Answer:**

**You're currently running in NATIVE MODE** (no Docker)

---

## üìä **Comparison Table:**

| Feature | Native Mode (`./run_local.sh`) | Docker Mode (`./setup.sh` or `docker-compose up`) |
|---------|-------------------------------|---------------------------------------------------|
| **What runs where** | Everything on host machine | Streamlit + PostgreSQL in containers |
| **Ollama** | ‚úÖ On host (localhost:11434) | ‚úÖ On host (accessed via host.docker.internal) |
| **Streamlit** | ‚úÖ On host (~/.local/bin/streamlit) | üê≥ In Docker container |
| **PostgreSQL** | ‚ùå Not used (JSON storage) | üê≥ In Docker container (pgvector) |
| **Python dependencies** | ‚úÖ Installed on host (~/.local/lib) | üê≥ Inside container |
| **Complexity** | ‚≠ê Simple | ‚≠ê‚≠ê‚≠ê Complex |
| **Startup time** | Fast (~5 seconds) | Slow (~30-60 seconds) |
| **Memory usage** | Low (~300MB for Streamlit) | Medium (~1GB for containers) |
| **Best for** | Development, testing, single user | Production, multi-user, isolation |

---

## üîç **Current Setup (Native Mode):**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Your Linux Machine          ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ   Ollama      ‚îÇ :11434          ‚îÇ
‚îÇ  ‚îÇ  (Host)       ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ          ‚îÇ                          ‚îÇ
‚îÇ          ‚îú‚îÄ‚îÄ> deepseek-coder:1.3b  ‚îÇ
‚îÇ          ‚îú‚îÄ‚îÄ> qwen2.5-coder:1.5b   ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ> qwen2.5-coder:3b     ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  Streamlit    ‚îÇ :8501           ‚îÇ
‚îÇ  ‚îÇ  (Host)       ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ          ‚îÇ                          ‚îÇ
‚îÇ          ‚Üì                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  Python App   ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  (src/app.py) ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ          ‚îÇ                          ‚îÇ
‚îÇ          ‚Üì                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  JSON Storage ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  (./data/)    ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Processes running:**
```bash
ollama serve              # PID 1749
streamlit run src/app.py  # PID 148547
```

---

## üê≥ **Docker Mode (Not Currently Running):**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Your Linux Machine          ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ   Ollama      ‚îÇ :11434          ‚îÇ
‚îÇ  ‚îÇ  (Host)       ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ          ‚îÇ                          ‚îÇ
‚îÇ          ‚Üë                          ‚îÇ
‚îÇ   via host.docker.internal         ‚îÇ
‚îÇ          ‚îÇ                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Docker Network            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Streamlit Container  ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ :8501                ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                      ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Python 3.11        ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Streamlit          ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - App code           ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ             ‚Üì              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ PostgreSQL Container ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ :5432                ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                      ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - pgvector extension ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Vector storage     ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Processes running (when Docker mode active):**
```bash
ollama serve                           # On host
docker compose up                      # Container manager
  ‚îî‚îÄ> streamlit-app container          # Streamlit in Docker
  ‚îî‚îÄ> postgres container                # PostgreSQL in Docker
```

---

## ü§î **Why Two Modes?**

### **Native Mode** (`./run_local.sh`) - **What you're using now**

**Pros:**
- ‚úÖ **Simple**: No Docker needed
- ‚úÖ **Fast**: Direct execution, no container overhead
- ‚úÖ **Easy debugging**: Direct access to logs
- ‚úÖ **Lightweight**: Only uses what you need
- ‚úÖ **Works on any Linux**: No Docker Desktop required

**Cons:**
- ‚ùå No PostgreSQL (uses JSON files for storage)
- ‚ùå No vector database optimizations
- ‚ùå Not isolated (Python packages on host)
- ‚ùå Single user only

**Best for:**
- Development
- Testing
- Personal use
- Learning
- Your current use case ‚úÖ

---

### **Docker Mode** (`docker-compose up`)

**Pros:**
- ‚úÖ **PostgreSQL + pgvector**: Real database with vector search
- ‚úÖ **Isolated**: Containers don't affect host
- ‚úÖ **Reproducible**: Same environment everywhere
- ‚úÖ **Multi-user**: Better for production
- ‚úÖ **Easy cleanup**: `docker-compose down` removes everything

**Cons:**
- ‚ùå **Complex**: Requires Docker knowledge
- ‚ùå **Slower**: Container startup time
- ‚ùå **More memory**: ~1GB+ for containers
- ‚ùå **Networking issues**: host.docker.internal on Linux can be tricky
- ‚ùå **Docker Desktop**: May require paid license on some systems

**Best for:**
- Production deployment
- Multiple users
- Advanced RAG with large document collections
- CI/CD pipelines
- Team environments

---

## üéØ **Which Mode Should You Use?**

### **Use Native Mode (`./run_local.sh`) if:**
- ‚úÖ You're developing/testing
- ‚úÖ Single user (just you)
- ‚úÖ Don't need PostgreSQL
- ‚úÖ Want simple and fast
- ‚úÖ Limited RAM/resources
- ‚úÖ **This is what you're doing now!** ‚úÖ

### **Use Docker Mode (`docker-compose up`) if:**
- ‚ùå Multiple users need access
- ‚ùå Need PostgreSQL + pgvector
- ‚ùå Want isolated environment
- ‚ùå Deploying to production
- ‚ùå Working in a team

---

## üîÑ **Switching Between Modes:**

### **Currently (Native) ‚Üí Docker:**
```bash
# Stop native mode
pkill streamlit
# (Ollama keeps running)

# Start Docker mode
docker-compose up -d

# Ollama on host will be accessed from containers via host.docker.internal
```

### **Docker ‚Üí Native (What we did today):**
```bash
# Stop Docker mode
docker-compose down

# Start native mode
./run_local.sh
```

---

## üìù **Data Storage:**

### **Native Mode:**
```
./data/
‚îú‚îÄ‚îÄ conversations/       # JSON files (conversation history)
‚îú‚îÄ‚îÄ documents/          # Uploaded files
‚îú‚îÄ‚îÄ uploads/            # Temporary uploads
‚îî‚îÄ‚îÄ db/                 # JSON-based vector storage
```

### **Docker Mode:**
```
PostgreSQL container:
‚îî‚îÄ> ai_playground database
    ‚îú‚îÄ‚îÄ conversations table
    ‚îú‚îÄ‚îÄ documents table
    ‚îî‚îÄ‚îÄ vectors table (with pgvector extension)

Plus same ./data/ folder for files
```

---

## ‚öôÔ∏è **Configuration Differences:**

### **Native Mode (.env):**
```bash
OLLAMA_HOST=http://localhost:11434     # Direct access
EMBED_MODEL=nomic-embed-text
EMBED_DIM=768
MCP_URL=http://localhost:8080
# No DATABASE_URL (uses JSON)
```

### **Docker Mode (docker-compose.yml env):**
```bash
OLLAMA_HOST=http://host.docker.internal:11434  # Via Docker network
DATABASE_URL=postgresql://ai_user:ai_password@postgres:5432/ai_playground
EMBED_MODEL=nomic-embed-text
EMBED_DIM=768
MCP_URL=http://host.docker.internal:8080
```

---

## üöÄ **Performance:**

| Metric | Native Mode | Docker Mode |
|--------|-------------|-------------|
| **Startup** | 5 seconds | 30-60 seconds |
| **Memory** | ~300MB | ~1GB+ |
| **LLM Speed** | Same (both use host Ollama) | Same |
| **DB queries** | Fast (JSON in memory) | Faster (PostgreSQL) |
| **Vector search** | Good (JSON) | Better (pgvector) |

---

## üí° **Key Insight:**

**Ollama ALWAYS runs on the host in both modes!**

Why?
- Ollama needs direct GPU/CPU access for inference
- Models are large (GBs) and stored on host
- Container overhead would slow down inference
- Host has your models already downloaded

**Only Streamlit and PostgreSQL run in Docker (when using Docker mode)**

---

## ‚úÖ **Your Current Setup (Native Mode):**

```bash
# Check what's running
ps aux | grep -E "(streamlit|ollama)"

# Should show:
# ollama serve                    <- On host
# streamlit run src/app.py        <- On host

# No Docker containers
docker ps
# Should show: (empty)
```

---

## üéì **Summary:**

You're using **Native Mode** because:
1. ‚úÖ Simpler for development
2. ‚úÖ Faster startup
3. ‚úÖ Less memory usage
4. ‚úÖ Easier to debug
5. ‚úÖ Don't need PostgreSQL yet

**When to switch to Docker:**
- Testing PostgreSQL integration
- Need advanced vector search
- Deploying to production
- Working with a team

For now, **stick with Native Mode** - it's perfect for your use case!

---

**Questions?** Ask away! üöÄ
